<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>2020-03-26_ConcreteGLM.utf8</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script src="site_libs/navigation-1.1/sourceembed.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
#rmd-source-code {
  display: none;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">[datadavidz]</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="blog.html">Blog</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="http://github.com/datadavidz">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-download-source" href="#">Download Rmd</a></li>
</ul>
</div>




</div>


<p><br></p>
<div id="generalized-linear-modeling-of-the-concrete-compressive-strength-dataset" class="section level2">
<h2><strong>Generalized Linear Modeling of the Concrete Compressive Strength Dataset</strong></h2>
<p><em>Posted on March 26, 2020</em></p>
<p>In this post, we will begin to use machine learning techniques for predicting compressive strength of formulations using the concrete dataset. In a previous post, we created a model using a conventional material modeling approach which resulted in an R<sup>2</sup> of 0.78. Here we will use a generalized linear model to predict compressive strength and compare the results with the conventional material model.</p>
<p>We will utilize the 3-stage machine learning approach promoted by Matt Dancho at Business-Science.io. He posted an excellent tutorial “Product Price Prediction: A Tidy Hyperparamter Tuning and Cross Validation Tutorial”. I haven’t found a better example of applying the tidymodel packages to develop a predictive model.</p>
<p>The 3-stage hyperparameter tuning process:<br />
1. <strong>Find Parameters:</strong> Use hyperparameter tuning on a “training dataset” that sections your training data into cross validation folds. The output of stage 1 is the parameter set.<br />
2. <strong>Compare and Select the Best Model:</strong> Evaluate the performance on a hidden “test dataset”. The output at Stage 2 is what we determined as the best model.<br />
3. <strong>Train Final Model:</strong> Once we have selected the best model, we train the full dataset. This model goes into production.</p>
<p>###Stage 1: Find Parameters Here we want to make different machine learning models and try them out by performing the following steps: - Initial Splitting: Separate into random training and test datasets - Preprocessing: Make a pipeline to transform raw data into a dataset ready for machine learning - Cross Validation Specification: Sample the training data into splits - Model Specification: Select model algorithms and identify key tuning parameters - Grid Specification: Set up a grid using wise parameter choices - Hyperparameter Tuning: Implement the tuning process</p>
<p>Initial splitting of the dataset into Training and Test Dataset Here we use the rsample package to create an 80/20 split. The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.</p>
<pre class="r"><code>set.seed(123)
concrete_initial_split &lt;- initial_split(concrete_tbl, prop = 0.80)
concrete_initial_split</code></pre>
<pre><code>## &lt;825/205/1030&gt;</code></pre>
<p>Preprocessing is accomplished by using the recipe package. The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning. The Concrete dataset actually doesn’t require much reformatting. The major issue was the lengthy column names which was addressed immediately after the dataset was imported. The dataset contained all numerical values and no missing data. Initially we will just center and scale the predictors before sending to the glmnet model.</p>
<pre class="r"><code>preprocessing_recipe &lt;- recipe(compressive_strength ~ ., data = training(concrete_initial_split)) %&gt;%
  
  #center and scale all numerical predictors
  step_center(all_predictors()) %&gt;%
  step_scale(all_predictors()) %&gt;%
  
  # #Remove unnecessary columns
  # step_rm(model) %&gt;%
  prep()

preprocessing_recipe</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          8
## 
## Training data contained 825 data points and no missing data.
## 
## Operations:
## 
## Centering for cement, blast_furnace_slag, fly_ash, ... [trained]
## Scaling for cement, blast_furnace_slag, fly_ash, ... [trained]</code></pre>
<p>The recipe is not actually applied to the dataset until you “bake” it. Here we apply the preprocessing pipeline and save it to a new tibble.</p>
<pre class="r"><code>concrete_training_preprocessed_tbl &lt;- preprocessing_recipe %&gt;% bake(training(concrete_initial_split))</code></pre>
<p>Cross validation folds are created in order to assess the performance of the model parameters. Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.</p>
<pre class="r"><code>set.seed(123)
concrete_cv_folds &lt;- training(concrete_initial_split) %&gt;%
  bake(preprocessing_recipe, new_data = .) %&gt;%
  vfold_cv(v = 5)

concrete_cv_folds</code></pre>
<pre><code>## #  5-fold cross-validation 
## # A tibble: 5 x 2
##   splits            id   
##   &lt;named list&gt;      &lt;chr&gt;
## 1 &lt;split [660/165]&gt; Fold1
## 2 &lt;split [660/165]&gt; Fold2
## 3 &lt;split [660/165]&gt; Fold3
## 4 &lt;split [660/165]&gt; Fold4
## 5 &lt;split [660/165]&gt; Fold5</code></pre>
<p>Model specifications are created using the parsnip package. Here we specify a linear regression model using the glmnet engine. glmnet uses an Elastic Net which combines LASSO and Ridge Regression techniques. This is a linear algorithm which may have difficulty with the skewed numeric data which is present in the Concrete dataset. Notice that the penalty and mixture parameters have been specified to be tuned.</p>
<pre class="r"><code>glmnet_model &lt;- linear_reg(
  mode = &quot;regression&quot;,
  penalty = tune(),
  mixture = tune()
) %&gt;%
  set_engine(&quot;glmnet&quot;)

glmnet_model</code></pre>
<pre><code>## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = tune()
##   mixture = tune()
## 
## Computational engine: glmnet</code></pre>
<p>Grid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy). Here we specify the parameter ranges and grid function using the dials package.</p>
<pre class="r"><code>glmnet_params &lt;- parameters(penalty(), mixture())
glmnet_params</code></pre>
<pre><code>## Collection of 2 parameters for tuning
## 
##       id parameter type object class
##  penalty        penalty    nparam[+]
##  mixture        mixture    nparam[+]</code></pre>
<p>Specify the grid function (max entropy, hypercube etc.). Here we make a grid of 20 values using the grid_max_entropy() function in the dials package. Since there are just 2 tuning parameters in this case, we can visualize the grid selections. Note the penalty parameter is on the log base 10 scale by default. The dials package helps us make smarter choices for the critical tuning parameters.</p>
<pre class="r"><code>set.seed(123)
glmnet_grid &lt;- grid_max_entropy(glmnet_params, size = 20)

glmnet_grid %&gt;%
  ggplot(aes(penalty, mixture)) +
  geom_point(color = &quot;steelblue&quot;, size = 3) +
  scale_x_log10() +
  theme_light() +
  labs(title = &quot;Max Entropy Grid&quot;, x = &quot;Penalty (log scale)&quot;, y = &quot;Mixture&quot;)</code></pre>
<p><img src="2020-03-26_ConcreteGLM_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Hyperparameter tuning is now performed using the tune_grid() function from the tune package. Here we specific the formula, model, resamples, grid and metrics. The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().</p>
<pre class="r"><code>glmnet_stage_1_cv_results_tbl &lt;- tune_grid(
  formula = compressive_strength ~ .,
  model = glmnet_model,
  resamples = concrete_cv_folds,
  grid = glmnet_grid,
  metrics = metric_set(mae, mape, rmse, rsq)#,
  #control = control_grid(verbose = TRUE)
)</code></pre>
<p>Identify the best hyperparameter values using the show_best() function.</p>
<pre class="r"><code>glmnet_stage_1_cv_results_tbl %&gt;% show_best(&quot;mae&quot;, n = 10, maximize = FALSE)</code></pre>
<pre><code>## # A tibble: 10 x 7
##     penalty mixture .metric .estimator  mean     n std_err
##       &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1 2.47e-10  0.666  mae     standard    8.42     5   0.135
##  2 1.31e- 7  0.546  mae     standard    8.42     5   0.135
##  3 1.48e- 4  0.996  mae     standard    8.42     5   0.135
##  4 4.95e- 8  0.753  mae     standard    8.42     5   0.135
##  5 1.70e- 3  0.590  mae     standard    8.42     5   0.135
##  6 1.49e- 6  0.973  mae     standard    8.42     5   0.135
##  7 2.31e- 9  0.921  mae     standard    8.42     5   0.135
##  8 1.10e- 5  0.699  mae     standard    8.42     5   0.135
##  9 1.69e- 9  0.0491 mae     standard    8.42     5   0.135
## 10 7.49e- 7  0.0747 mae     standard    8.42     5   0.135</code></pre>
<p>###Stage 2: Compare and Select the Best Model</p>
<p>Select the best parameters based on the lowest mean absolute error.</p>
<pre class="r"><code>params_glmnet_best &lt;- glmnet_stage_1_cv_results_tbl %&gt;% select_best(&quot;mae&quot;, maximize = FALSE)
params_glmnet_best</code></pre>
<pre><code>## # A tibble: 1 x 2
##    penalty mixture
##      &lt;dbl&gt;   &lt;dbl&gt;
## 1 2.47e-10   0.666</code></pre>
<p>Finalize the model with the best parameters.</p>
<pre class="r"><code>glmnet_stage_2_model &lt;- glmnet_model %&gt;%
  finalize_model(parameters = params_glmnet_best)

glmnet_stage_2_model</code></pre>
<pre><code>## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 2.46944747219784e-10
##   mixture = 0.666244992753491
## 
## Computational engine: glmnet</code></pre>
<p>Define a helper function to calculate the performance on the test dataset</p>
<pre class="r"><code>calc_test_metrics &lt;- function(formula, model_spec, recipe, split) {
  
  train_processed &lt;- training(split) %&gt;% bake(recipe, new_data = .)
  test_processed &lt;- testing(split) %&gt;% bake(recipe, new_data = .)
  
  target_expr &lt;- recipe %&gt;%
    pluck(&quot;last_term_info&quot;) %&gt;%
    filter(role == &quot;outcome&quot;) %&gt;%
    pull(variable) %&gt;%
    sym()
  
  model_spec %&gt;%
    fit(formula = as.formula(formula),
        data = train_processed) %&gt;%
    predict(new_data = test_processed) %&gt;%
    bind_cols(testing(split)) %&gt;%
    metrics(!! target_expr, .pred)
}</code></pre>
<p>Calculate the test performance on the glmnet model.</p>
<pre class="r"><code>glmnet_stage_2_metrics &lt;- calc_test_metrics(
  formula = compressive_strength ~ .,
  model_spec = glmnet_stage_2_model,
  recipe = preprocessing_recipe,
  split = concrete_initial_split
)

glmnet_stage_2_metrics</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard      10.3  
## 2 rsq     standard       0.609
## 3 mae     standard       8.12</code></pre>
<p>###Stage 3: Train Final Model</p>
<pre class="r"><code>model_final &lt;- glmnet_stage_2_model %&gt;%
  fit(compressive_strength ~ ., data = bake(preprocessing_recipe, new_data = concrete_tbl))</code></pre>
<p>Which Features are most important?</p>
<pre class="r"><code>vip(model_final, aesthetics = list(fill = &quot;steelblue&quot;)) +
  labs(title = &quot;GLMNET Model Importance - Compressive Strength (MPa) Prediction&quot;) +
  theme_bw()</code></pre>
<p><img src="2020-03-26_ConcreteGLM_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>

<div id="rmd-source-code">---
output: 
  html_document:
    code_download: true
    includes:
      after_body: footer.html
---
<br>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(knitr)
library(readxl)
library(tidyverse)

#Tidymodels
library(tune)
library(dials)
library(parsnip)
library(rsample)
library(recipes)
#library(textrecipes)
library(yardstick)
library(vip)
```

## **Generalized Linear Modeling of the Concrete Compressive Strength Dataset**
*Posted on March 26, 2020*

In this post, we will begin to use machine learning techniques for predicting compressive strength of formulations using the concrete dataset.  In a previous post, we created a model using a conventional material modeling approach which resulted in an R^2^ of 0.78.  Here we will use a generalized linear model to predict compressive strength and compare the results with the conventional material model.

```{r, include=FALSE}
library(readxl)
library(tidyverse)

filename <- "Concrete_Data.xls"

folder <- "./data/"
numberCols <- 9 #total number of columns in spreadsheet

colTypes <- rep("numeric", numberCols)
concrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)

concrete_tbl <- concrete_tbl %>%
  rename(cement = starts_with("Cement")) %>%
  rename(blast_furnace_slag = starts_with("Blast")) %>%
  rename(fly_ash = starts_with("Fly Ash")) %>%
  rename(water = starts_with("Water")) %>%
  rename(superplasticizer = starts_with("Super")) %>%
  rename(coarse_aggregate = starts_with("Coarse")) %>%
  rename(fine_aggregate = starts_with("Fine")) %>%
  rename(age = starts_with("Age")) %>%
  rename(compressive_strength = starts_with("Concrete"))
```

We will utilize the 3-stage machine learning approach promoted by Matt Dancho at Business-Science.io.  He posted an excellent tutorial "Product Price Prediction: A Tidy Hyperparamter Tuning and Cross Validation Tutorial".  I haven't found a better example of applying the tidymodel packages to develop a predictive model.

The 3-stage hyperparameter tuning process:  
1. **Find Parameters:** Use hyperparameter tuning on a "training dataset" that sections your training data into cross validation folds.  The output of stage 1 is the parameter set.  
2. **Compare and Select the Best Model:** Evaluate the performance on a hidden "test dataset".  The output at Stage 2 is what we determined as the best model.  
3. **Train Final Model:** Once we have selected the best model, we train the full dataset.  This model goes into production.  


###Stage 1: Find Parameters
Here we want to make different machine learning models and try them out by performing the following steps:
- Initial Splitting: Separate into random training and test datasets
- Preprocessing: Make a pipeline to transform raw data into a dataset ready for machine learning
- Cross Validation Specification: Sample the training data into splits
- Model Specification: Select model algorithms and identify key tuning parameters
- Grid Specification: Set up a grid using wise parameter choices
- Hyperparameter Tuning: Implement the tuning process

Initial splitting of the dataset into Training and Test Dataset  Here we use the rsample package to create an 80/20 split.  The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.
```{r}
set.seed(123)
concrete_initial_split <- initial_split(concrete_tbl, prop = 0.80)
concrete_initial_split
```

Preprocessing is accomplished by using the recipe package.  The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning.  The Concrete dataset actually doesn't require much reformatting.  The major issue was the lengthy column names which was addressed immediately after the dataset was imported.  The dataset contained all numerical values and no missing data.  Initially we will just center and scale the predictors before sending to the glmnet model.
```{r}
preprocessing_recipe <- recipe(compressive_strength ~ ., data = training(concrete_initial_split)) %>%
  
  #center and scale all numerical predictors
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  
  # #Remove unnecessary columns
  # step_rm(model) %>%
  prep()

preprocessing_recipe
```

The recipe is not actually applied to the dataset until you "bake" it.  Here we apply the preprocessing pipeline and save it to a new tibble.
```{r}
concrete_training_preprocessed_tbl <- preprocessing_recipe %>% bake(training(concrete_initial_split))
```

Cross validation folds are created in order to assess the performance of the model parameters.  Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.
```{r}
set.seed(123)
concrete_cv_folds <- training(concrete_initial_split) %>%
  bake(preprocessing_recipe, new_data = .) %>%
  vfold_cv(v = 5)

concrete_cv_folds
```

Model specifications are created using the parsnip package.  Here we specify a linear regression model using the glmnet engine.  glmnet uses an Elastic Net which combines LASSO and Ridge Regression techniques.  This is a linear algorithm which may have difficulty with the skewed numeric data which is present in the Concrete dataset.  Notice that the penalty and mixture parameters have been specified to be tuned.
```{r}
glmnet_model <- linear_reg(
  mode = "regression",
  penalty = tune(),
  mixture = tune()
) %>%
  set_engine("glmnet")

glmnet_model
```

Grid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy).  Here we specify the parameter ranges and grid function using the dials package.
```{r}
glmnet_params <- parameters(penalty(), mixture())
glmnet_params
```

Specify the grid function (max entropy, hypercube etc.).  Here we make a grid of 20 values using the grid_max_entropy() function in the dials package.  Since there are just 2 tuning parameters in this case, we can visualize the grid selections.  Note the penalty parameter is on the log base 10 scale by default.  The dials package helps us make smarter choices for the critical tuning parameters.
```{r}
set.seed(123)
glmnet_grid <- grid_max_entropy(glmnet_params, size = 20)

glmnet_grid %>%
  ggplot(aes(penalty, mixture)) +
  geom_point(color = "steelblue", size = 3) +
  scale_x_log10() +
  theme_light() +
  labs(title = "Max Entropy Grid", x = "Penalty (log scale)", y = "Mixture")
```

Hyperparameter tuning is now performed using the tune_grid() function from the tune package.  Here we specific the formula, model, resamples, grid and metrics.  The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().
```{r}
glmnet_stage_1_cv_results_tbl <- tune_grid(
  formula = compressive_strength ~ .,
  model = glmnet_model,
  resamples = concrete_cv_folds,
  grid = glmnet_grid,
  metrics = metric_set(mae, mape, rmse, rsq)#,
  #control = control_grid(verbose = TRUE)
)
```

Identify the best hyperparameter values using the show_best() function.
```{r}
glmnet_stage_1_cv_results_tbl %>% show_best("mae", n = 10, maximize = FALSE)
```

###Stage 2: Compare and Select the Best Model

Select the best parameters based on the lowest mean absolute error.
```{r}
params_glmnet_best <- glmnet_stage_1_cv_results_tbl %>% select_best("mae", maximize = FALSE)
params_glmnet_best
```

Finalize the model with the best parameters.
```{r}
glmnet_stage_2_model <- glmnet_model %>%
  finalize_model(parameters = params_glmnet_best)

glmnet_stage_2_model
```

Define a helper function to calculate the performance on the test dataset
```{r}
calc_test_metrics <- function(formula, model_spec, recipe, split) {
  
  train_processed <- training(split) %>% bake(recipe, new_data = .)
  test_processed <- testing(split) %>% bake(recipe, new_data = .)
  
  target_expr <- recipe %>%
    pluck("last_term_info") %>%
    filter(role == "outcome") %>%
    pull(variable) %>%
    sym()
  
  model_spec %>%
    fit(formula = as.formula(formula),
        data = train_processed) %>%
    predict(new_data = test_processed) %>%
    bind_cols(testing(split)) %>%
    metrics(!! target_expr, .pred)
}
```

Calculate the test performance on the glmnet model.
```{r}
glmnet_stage_2_metrics <- calc_test_metrics(
  formula = compressive_strength ~ .,
  model_spec = glmnet_stage_2_model,
  recipe = preprocessing_recipe,
  split = concrete_initial_split
)

glmnet_stage_2_metrics
```

###Stage 3: Train Final Model
```{r}
model_final <- glmnet_stage_2_model %>%
  fit(compressive_strength ~ ., data = bake(preprocessing_recipe, new_data = concrete_tbl))
```

Which Features are most important?
```{r}
vip(model_final, aesthetics = list(fill = "steelblue")) +
  labs(title = "GLMNET Model Importance - Compressive Strength (MPa) Prediction") +
  theme_bw()
```

</div>
&nbsp;
<hr />
<p style="text-align: center;"> A work by <a href="https://github.com/datadavidz/">datadavidz</a></p>
&nbsp;
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://datadavidz.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeSourceEmbed("2020-03-26_ConcreteGLM.Rmd");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
